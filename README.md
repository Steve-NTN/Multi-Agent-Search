# Multi-Agent-Search

## Mô tả cách hiểu về câu hỏi và cách giải:
- **ReflexAgent**: 
  - Hoàn thiện ReflexAgent trong multiAgents.py. Mã tác nhân phản xạ được cung cấp cung cấp một số ví dụ hữu ích về các phương thức truy vấn thông tin GameState. Một phản xạ có khả năng sẽ phải xem xét cả vị trí thức ăn và vị trí ma để thực hiện được tốt.
  - Tạo các biến lưu trữ vị trí hiện tại của Pacman, chuỗi thức ăn hiện tại, vị trí hiện tại của ma, và thời gian con ma sợ hiện tại. Khởi tạo biến khoảng cách. Tạo vòng lặp trạng thái của con ma trong các trạng thái mới của con ma: lưu vị trí của con ma, nếu mà vị trí ấy mà bằng với vị trí mà Pacman sẽ đi thì trả về giá trị "-inf" và kết thúc hàm. Nếu vòng lặp trên không bị trả về, tiếp tục tạo 1 vòng lặp mới biến food chạy hết danh sách các thức ăn hiện tại: nếu biến khoảng cách lớn hơn khoảng cách của vị trí mới Pacman đến thức ăn thì đặt lại giá trị khoảng cách, nếu không thì bỏ qua. Nếu Pacman bị đứng yên trả về giá trị "-inf". Trả về 1/(1+khoảng cách) nếu thỏa mãn các điều kiện trên.

- **Minimax**: 
  - Viết một Agent tìm kiếm trong lớp MinimaxAgent trong multiAgents.py. Agent này sẽ hoạt động với bất kì số lượng ma nào, vì thế ta phải viết một thuật toán tổng quát hơn. Cây minimax của bạn sẽ có nhiều lớp tối thiểu cho mỗi lớp tối đa. Mã của bạn cũng nên mở rộng cây đến độ sâu tùy ý.
  - Xét chỉ số Agent, nếu nó lớn hơn chỉ số Agent của trạng thái game thì reset nó về 0 và tăng chiều sâu duyệt lên 1. Điều kiện thứ 2 là nếu chiều sâu duyệt chạm tới chiều sâu max (self) trả về hàm đánh giá trạng thái. Nếu điều kiện thứ 2 sai thì lại xét chỉ số Agent có bằng chỉ số max không, nếu có thì trả về hàm max value: Hàm max_value nhận 3 tham số đầu vào (trạng thái game, chỉ số Agent, và chiều sâu hiện tại), hàm sẽ kiểm tra trạng thái game có đang thắng hay đang thua không, nếu có thì trả về hàm đánh giá trạng thái, nếu không thì tạo biến lưu trữ các hành động hợp lệ của chỉ số Agent. Duyệt vòng lặp các hành động hợp lệ: nếu mà hành động nào là dừng lại thì trả về biến giá trị (value) nếu không thì tạo biến *successor* lưu trữ cái Agent kế tiếp từ trạng thái game, tạo biến *temp* lưu giá trị minimax của self với đầu vào là Agent kế tiếp (successor), chỉ số Agent thêm 1 giá trị và độ sâu hiện tại. Nếu mà biến *temp* > *value* thì giá trị biến *value* sẽ là *temp*, và giá trị biến *actionValue* sẽ là giá trị biến *actions*. Kết thúc vòng lặp, xét nếu độ sâu hiện tại là 0 thì trả về biến *actionValue* nếu không thì trả về *value* đã tính ở trên.
  - Hàm min_value tương tự hàm max_value, điều khác là lúc so sánh giá trị biến *value* và *temp* thì max_value lấy giá trị lớn hơn, còn min_value lấy giá trị bé hơn. Hàm getAction trả về hàm minimax có đầu vào là trạng thái game và 0, 0 là chỉ số Agent và độ sâu.
  
- **Alpha-Beta**: 
  - Tạo một Agent mới sử dụng alpha-beta để khám phá cây minimax hiệu quả hơn. Thuật toán sẽ phải tổng quát hơn một chút so với mã giả đã cho. Các AlphaBetaAgent giá trị minimax phải giiống hệt các MinimaxAgent giá trị minimax, mặc dù các hành động mà nó chọn có thể khác nhau do hành vi phá vỡ liên kết khác nhau. 
   - Hàm alpha_beta_value có 5 tham số đầu vào là trạng thái game, chỉ số Agent, độ sâu duyệt và giá trị cận dưới alpha và cận trên beta. Xét chỉ số Agent, nếu nó lớn hơn chỉ số Agent của trạng thái game thì reset nó về 0 và tăng chiều sâu duyệt lên 1. Điều kiện thứ 2 là nếu chiều sâu duyệt chạm tới chiều sâu max (self) trả về hàm đánh giá trạng thái. Nếu điều kiện thứ 2 sai thì lại xét chỉ số Agent có bằng chỉ số max không, nếu có thì trả về hàm max value: Hàm max_value nhận 3 tham số đầu vào (trạng thái game, chỉ số Agent, và chiều sâu hiện tại), hàm sẽ kiểm tra trạng thái game có đang thắng hay đang thua không, nếu có thì trả về hàm đánh giá trạng thái, nếu không thì tạo biến lưu trữ các hành động hợp lệ của chỉ số Agent. Duyệt vòng lặp các hành động hợp lệ: nếu mà hành động nào là dừng lại thì trả về biến giá trị (value) nếu không thì tạo biến *successor* lưu trữ cái Agent kế tiếp từ trạng thái game, tạo biến *temp* lưu giá trị minimax của self với đầu vào là Agent kế tiếp (successor), chỉ số Agent thêm 1 giá trị và độ sâu hiện tại. Nếu mà biến *temp* > *value* thì giá trị biến *value* sẽ là *temp*, và giá trị biến *actionValue* sẽ là giá trị biến *legalactions*. Kết thúc vòng lặp, xét nếu độ sâu hiện tại là 0 thì trả về biến *actionValue* nếu không thì trả về *value* đã tính ở trên. Xét tiếp giá trị *value* với giá trị cận trên beta, nếu *value* > *beta* thì trả về *value*. Biến *alpha* sẽ bằng số lớn hơn trong 2 giá trị biến *alpha* và *value*. Nếu độ sâu bằng 0 thì trả về *actionValue* nếu không trả về *value*.
   - Hàm min_value tương tự max_value, khác điểm là lúc so sánh temp và value, nó sẽ lấy giá trị *temp* cho *value* nếu *temp* < *value*. Xét *value* mà lớn hơn giá trị *alpha* thì trả về *value* và beta sẽ là min của 2 giá trị *beta* va *value*.
- **Expectimax**: 
  - Minimax và alpha-beta là tuyệt vời, nhưng cả hai đều cho thấy rằng ta đang chơi với một kẻ thù đưa ra quyết định tối ưu. Như bất cứ ai đã từng giành được tic-tac-toe đều có thể nói với bạn, điều này không phải lúc nào cũng đúng. Trong câu hỏi này, bạn sẽ thực hiện ExpectimaxAgent, rất hữu ích cho việc mô hình hóa hành vi xác suất của các tác nhân có thể đưa ra lựa chọn dưới mức tối ưu.
  - Hàm expectimax_value tương tự như hàm minimax_value ở question 2. Hàm max_value cũng tương tự ở minimax_value. 
  - Hàm exp_value xét trạng thái game có phải đang là trạng thái thua hay thắng không. Nếu đúng thì trả về hàm đánh giá trạng thái, nếu không thì thực hiện tiếp:
  biến *probValue* mang giá trị = 1 / (số lượng của các hành động hợp lệ của chỉ số Agent). Vòng lặp duyết hết các hành động hợp lệ của chỉ số Agent: nếu nó khác hành động đứng yên thì tính ra giá trị 2 biến *successor* và *temp* như cái question ở trên. Biến *value* sẽ mang giá trị mới bằng tổng chính nó với tích của của *temp* và *probValue*. Trả về *value*.
- **Evaluation Funtion**: 
  - Viết một hàm đánh giá tốt hơn cho Paman trong betterEvaluationFunction. Hàm đánh giá sẽ đánh giá các trạng thái, thay vì các hành động như hàm đánh giá phản xạ đã làm ở question 1. Với tìm kiếm độ sâu 2, chức năng đánh giá của bạn xóa smallClassic với một bóng ma ngẫu nhiên hơn một nửa thời ian và vẫn chạt ở tốc độ hớp lý. Đối với chức năng đánh giá tác nhân phản xạ của bạn, bạn có thể muốn sử dụng đối ứng của các giá trị quan trọng (chẳng hạn như khoảng cách đến thực phẩm) thay vì chính các giá trị. Một cách bạn có thể muốn viết chức năng đánh giá của mình là sử dụng kết hợp các tính năng tuyến tính. Nghĩa là, tính toán các giá trị cho các tính năng về trạng thái mà bạn cho là quan trọng, sau đó kết hợp các tính năng đó bằng cách nhân chúng với các giá trị khác nhau và thêm kết quả lại với nhau. Bạn có thể quyết định nhân gì mỗi tính năng dựa trên mức độ quan trọng của bạn.
  - Tính lượng thức ăn còn lại: 1 / (số thức ăn hiện tại + 1) và số ma đang sợ là 0.
Duyệt trạng thái của các con ma: nếu vị trí hiện tại của Pacman mà trùng với vị trí của con ma thì trả về giá trị "-inf" nếu không thì khoảng cách vs con ma sẽ là min của chính nó và khoảng cách hiện tại của Pacman với con ma. Nếu thời gian sợ khác 0 thì biến *scaredGhost* đã được khởi tạo là 0 sẽ tăng thêm 1 đơn vị. Dùng vòng duyệt tìm ra giá trị nhỏ nhất của khoảng cách từ vị trí Pacman đến trạng thái capsule. *ghostDist* sẽ là: 1 / (1 + ( *ghostDist* / số trạng thái của con ma hiện tại), *capDist* mang giá trị là: 1 / (1 + số lượng capsules hiện tại), số con ma sợ = 1 / ( 1 + *scaredGhosts*). Cuối cùng trả về tổng điểm hiện tại với lượng thức ăn còn lại, *ghostDist* và *capDist* đã tính ở trên. 
